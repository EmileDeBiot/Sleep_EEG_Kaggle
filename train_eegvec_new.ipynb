{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg2vec.train.train import train\n",
    "from eeg2vec.data_loader import get_dataloader\n",
    "from eeg2vec.models.eeg2vec import EEG2Vec\n",
    "from eeg2vec.contrastive_loss import ContrastiveLoss\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's load the training data\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "ROOT_PATH = Path(\"train/\")\n",
    "training_data = [(np.load(ROOT_PATH / f\"data_{i}.npy\"),np.load(ROOT_PATH / f\"target_{i}.npy\")) for i in range(4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bandpass filter to the data\n",
    "training_data_filtered = [(butter_bandpass_filter(data, 0.1, 18, 100), target) for data, target in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get the point that maps to a label\n",
    "\n",
    "def reshape_array_into_windows(x, sample_rate, window_duration_in_seconds):\n",
    "    \"\"\"\n",
    "    Reshape the data into an array of shape (C, T, window) where 'window' contains\n",
    "    the points corresponding to 'window_duration' seconds of data.\n",
    "\n",
    "    Parameters:\n",
    "    x (numpy array): The input data array.\n",
    "    sample_rate (int): The number of samples per second.\n",
    "    window_duration_in_seconds (float): The duration of each window in seconds.\n",
    "\n",
    "    Returns:\n",
    "    reshaped_x (numpy array): The reshaped array with shape (C, T, window).\n",
    "    \"\"\"\n",
    "    # Calculate the number of samples in one window\n",
    "    window_size = int(window_duration_in_seconds * sample_rate)\n",
    "    \n",
    "    # Ensure the total length of x is a multiple of window_size\n",
    "    total_samples = x.shape[-1]\n",
    "    if total_samples % window_size != 0:\n",
    "        # Truncate or pad x to make it divisible by window_size\n",
    "        x = x[..., :total_samples - (total_samples % window_size)]\n",
    "    # Reshape x into (C, T, window)\n",
    "    reshaped_x = x.reshape(x.shape[0], -1, window_size)\n",
    "\n",
    "    return reshaped_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load and reshape all the data\n",
    "all_data = []\n",
    "all_targets = []\n",
    "# We need to have\n",
    "# data of Shape: [num_samples, num_channels (5), sequence_length]\n",
    "# labels of Shape: [num_samples, 5]\n",
    "\n",
    "for data, target in training_data:\n",
    "    reshaped_data = reshape_array_into_windows(data, 250, 2)\n",
    "    reshaped_data = reshaped_data.transpose(1, 0, 2)\n",
    "    target = target.reshape(-1, 5)\n",
    "    all_data.append(reshaped_data)\n",
    "    all_targets.append(target)\n",
    "\n",
    "all_data = np.concatenate(all_data, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52351, 5, 500)\n"
     ]
    }
   ],
   "source": [
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = all_data[:10000], all_targets[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train EEG2VEC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emile\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\Emile\\AppData\\Local\\Temp\\ipykernel_40228\\1988608007.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eeg2vec_model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"eeg2vec/data/saved_models/eeg2vec_8_2_5_2_11dec_pretrained.pth\"\n",
    "eeg2vec_model = EEG2Vec(8,2,5,2)\n",
    "eeg2vec_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = get_dataloader(X_train, y_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 completed.\n",
      "Epoch 2/100 completed.\n",
      "Epoch 3/100 completed.\n",
      "Epoch 4/100 completed.\n",
      "Epoch 5/100 completed.\n",
      "Epoch 6/100 completed.\n",
      "Epoch 7/100 completed.\n",
      "Epoch 8/100 completed.\n",
      "Epoch 9/100 completed.\n",
      "Epoch 10/100 completed.\n",
      "Epoch 11/100 completed.\n",
      "Epoch 12/100 completed.\n",
      "Epoch 13/100 completed.\n",
      "Epoch 14/100 completed.\n",
      "Epoch 15/100 completed.\n",
      "Epoch 16/100 completed.\n",
      "Epoch 17/100 completed.\n",
      "Epoch 18/100 completed.\n",
      "Epoch 19/100 completed.\n",
      "Epoch 20/100 completed.\n",
      "Epoch 21/100 completed.\n",
      "Epoch 22/100 completed.\n",
      "Epoch 23/100 completed.\n",
      "Epoch 24/100 completed.\n",
      "Epoch 25/100 completed.\n",
      "Epoch 26/100 completed.\n",
      "Epoch 27/100 completed.\n",
      "Epoch 28/100 completed.\n",
      "Epoch 29/100 completed.\n",
      "Epoch 30/100 completed.\n",
      "Epoch 31/100 completed.\n",
      "Epoch 32/100 completed.\n",
      "Epoch 33/100 completed.\n",
      "Epoch 34/100 completed.\n",
      "Epoch 35/100 completed.\n",
      "Epoch 36/100 completed.\n",
      "Epoch 37/100 completed.\n",
      "Epoch 38/100 completed.\n",
      "Epoch 39/100 completed.\n",
      "Epoch 40/100 completed.\n",
      "Epoch 41/100 completed.\n",
      "Epoch 42/100 completed.\n",
      "Epoch 43/100 completed.\n",
      "Epoch 44/100 completed.\n",
      "Epoch 45/100 completed.\n",
      "Epoch 46/100 completed.\n",
      "Epoch 47/100 completed.\n",
      "Epoch 48/100 completed.\n",
      "Epoch 49/100 completed.\n",
      "Epoch 50/100 completed.\n",
      "Epoch 51/100 completed.\n",
      "Epoch 52/100 completed.\n",
      "Epoch 53/100 completed.\n",
      "Epoch 54/100 completed.\n",
      "Epoch 55/100 completed.\n",
      "Epoch 56/100 completed.\n",
      "Epoch 57/100 completed.\n",
      "Epoch 58/100 completed.\n",
      "Epoch 59/100 completed.\n",
      "Epoch 60/100 completed.\n",
      "Epoch 61/100 completed.\n",
      "Epoch 62/100 completed.\n",
      "Epoch 63/100 completed.\n",
      "Epoch 64/100 completed.\n",
      "Epoch 65/100 completed.\n",
      "Epoch 66/100 completed.\n",
      "Epoch 67/100 completed.\n",
      "Epoch 68/100 completed.\n",
      "Epoch 69/100 completed.\n",
      "Epoch 70/100 completed.\n",
      "Epoch 71/100 completed.\n",
      "Epoch 72/100 completed.\n",
      "Epoch 73/100 completed.\n",
      "Epoch 74/100 completed.\n",
      "Epoch 75/100 completed.\n",
      "Epoch 76/100 completed.\n",
      "Epoch 77/100 completed.\n",
      "Epoch 78/100 completed.\n",
      "Epoch 79/100 completed.\n",
      "Epoch 80/100 completed.\n",
      "Epoch 81/100 completed.\n",
      "Epoch 82/100 completed.\n",
      "Epoch 83/100 completed.\n",
      "Epoch 84/100 completed.\n",
      "Epoch 85/100 completed.\n",
      "Epoch 86/100 completed.\n",
      "Epoch 87/100 completed.\n",
      "Epoch 88/100 completed.\n",
      "Epoch 89/100 completed.\n",
      "Epoch 90/100 completed.\n",
      "Epoch 91/100 completed.\n",
      "Epoch 92/100 completed.\n",
      "Epoch 93/100 completed.\n",
      "Epoch 94/100 completed.\n",
      "Epoch 95/100 completed.\n",
      "Epoch 96/100 completed.\n",
      "Epoch 97/100 completed.\n",
      "Epoch 98/100 completed.\n",
      "Epoch 99/100 completed.\n",
      "Epoch 100/100 completed.\n"
     ]
    }
   ],
   "source": [
    "eeg2vec_model = eeg2vec_model.to(device)\n",
    "train(eeg2vec_model, data_loader, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(eeg2vec_model.state_dict(), 'eeg2vec/data/saved_models/eeg2vec_8_2_5_2_11dec_10000points.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
