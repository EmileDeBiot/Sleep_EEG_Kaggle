{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg2vec.train.train import train\n",
    "from eeg2vec.data_loader import get_dataloader\n",
    "from eeg2vec.models.eeg2vec import EEG2Vec\n",
    "from eeg2vec.contrastive_loss import ContrastiveLoss\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's load the training data\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "ROOT_PATH = Path(\"train/\")\n",
    "training_data = [(np.load(ROOT_PATH / f\"data_{i}.npy\"),np.load(ROOT_PATH / f\"target_{i}.npy\")) for i in range(4)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7712740)\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get the point that maps to a label\n",
    "\n",
    "def reshape_array_into_windows(x, sample_rate, window_duration_in_seconds):\n",
    "    \"\"\"\n",
    "    Reshape the data into an array of shape (C, T, window) where 'window' contains\n",
    "    the points corresponding to 'window_duration' seconds of data.\n",
    "\n",
    "    Parameters:\n",
    "    x (numpy array): The input data array.\n",
    "    sample_rate (int): The number of samples per second.\n",
    "    window_duration_in_seconds (float): The duration of each window in seconds.\n",
    "\n",
    "    Returns:\n",
    "    reshaped_x (numpy array): The reshaped array with shape (C, T, window).\n",
    "    \"\"\"\n",
    "    # Calculate the number of samples in one window\n",
    "    window_size = int(window_duration_in_seconds * sample_rate)\n",
    "    \n",
    "    # Ensure the total length of x is a multiple of window_size\n",
    "    total_samples = x.shape[-1]\n",
    "    if total_samples % window_size != 0:\n",
    "        # Truncate or pad x to make it divisible by window_size\n",
    "        x = x[..., :total_samples - (total_samples % window_size)]\n",
    "    # Reshape x into (C, T, window)\n",
    "    reshaped_x = x.reshape(x.shape[0], -1, window_size)\n",
    "\n",
    "    return reshaped_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load and reshape all the data\n",
    "all_data = []\n",
    "all_targets = []\n",
    "# We need to have\n",
    "# data of Shape: [num_samples, num_channels (5), sequence_length]\n",
    "# labels of Shape: [num_samples, 5]\n",
    "\n",
    "for data, target in training_data:\n",
    "    reshaped_data = reshape_array_into_windows(data, 250, 2)\n",
    "    reshaped_data = reshaped_data.transpose(1, 0, 2)\n",
    "    target = target.reshape(-1, 5)\n",
    "    all_data.append(reshaped_data)\n",
    "    all_targets.append(target)\n",
    "\n",
    "all_data = np.concatenate(all_data, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52351, 5, 500)\n",
      "(52351, 5)\n"
     ]
    }
   ],
   "source": [
    "print(all_data.shape)\n",
    "print(all_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_data[1200:2000]\n",
    "labels = all_targets[1200:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split training data for embeddings and XGBoost\n",
    "X_train_embeddings, X_train_xgboost, y_train_embeddings, y_train_xgboost = train_test_split(X_train_full, y_train_full, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 5, 500) (320, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_embeddings.shape, y_train_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = get_dataloader(X_train_embeddings, y_train_embeddings, batch_size=100, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EEG2Vec(\n",
       "  (cnn_encoder): CNNEncoder(\n",
       "    (conv_layers): Sequential(\n",
       "      (0): Conv1d(5, 16, kernel_size=(2,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv1d(16, 16, kernel_size=(2,), stride=(1,))\n",
       "      (4): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=2, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=2, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=16, out_features=2, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2, out_features=16, bias=True)\n",
       "          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EEG2Vec(16, 2, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu113\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 completed.\n",
      "Epoch 2/100 completed.\n",
      "Epoch 3/100 completed.\n",
      "Epoch 4/100 completed.\n",
      "Epoch 5/100 completed.\n",
      "Epoch 6/100 completed.\n",
      "Epoch 7/100 completed.\n",
      "Epoch 8/100 completed.\n",
      "Epoch 9/100 completed.\n",
      "Epoch 10/100 completed.\n",
      "Epoch 11/100 completed.\n",
      "Epoch 12/100 completed.\n",
      "Epoch 13/100 completed.\n",
      "Epoch 14/100 completed.\n",
      "Epoch 15/100 completed.\n",
      "Epoch 16/100 completed.\n",
      "Epoch 17/100 completed.\n",
      "Epoch 18/100 completed.\n",
      "Epoch 19/100 completed.\n",
      "Epoch 20/100 completed.\n",
      "Epoch 21/100 completed.\n",
      "Epoch 22/100 completed.\n",
      "Epoch 23/100 completed.\n",
      "Epoch 24/100 completed.\n",
      "Epoch 25/100 completed.\n",
      "Epoch 26/100 completed.\n",
      "Epoch 27/100 completed.\n",
      "Epoch 28/100 completed.\n",
      "Epoch 29/100 completed.\n",
      "Epoch 30/100 completed.\n",
      "Epoch 31/100 completed.\n",
      "Epoch 32/100 completed.\n",
      "Epoch 33/100 completed.\n",
      "Epoch 34/100 completed.\n",
      "Epoch 35/100 completed.\n",
      "Epoch 36/100 completed.\n",
      "Epoch 37/100 completed.\n",
      "Epoch 38/100 completed.\n",
      "Epoch 39/100 completed.\n",
      "Epoch 40/100 completed.\n",
      "Epoch 41/100 completed.\n",
      "Epoch 42/100 completed.\n",
      "Epoch 43/100 completed.\n",
      "Epoch 44/100 completed.\n",
      "Epoch 45/100 completed.\n",
      "Epoch 46/100 completed.\n",
      "Epoch 47/100 completed.\n",
      "Epoch 48/100 completed.\n",
      "Epoch 49/100 completed.\n",
      "Epoch 50/100 completed.\n",
      "Epoch 51/100 completed.\n",
      "Epoch 52/100 completed.\n",
      "Epoch 53/100 completed.\n",
      "Epoch 54/100 completed.\n",
      "Epoch 55/100 completed.\n",
      "Epoch 56/100 completed.\n",
      "Epoch 57/100 completed.\n",
      "Epoch 58/100 completed.\n",
      "Epoch 59/100 completed.\n",
      "Epoch 60/100 completed.\n",
      "Epoch 61/100 completed.\n",
      "Epoch 62/100 completed.\n",
      "Epoch 63/100 completed.\n",
      "Epoch 64/100 completed.\n",
      "Epoch 65/100 completed.\n",
      "Epoch 66/100 completed.\n",
      "Epoch 67/100 completed.\n",
      "Epoch 68/100 completed.\n",
      "Epoch 69/100 completed.\n",
      "Epoch 70/100 completed.\n",
      "Epoch 71/100 completed.\n",
      "Epoch 72/100 completed.\n",
      "Epoch 73/100 completed.\n",
      "Epoch 74/100 completed.\n",
      "Epoch 75/100 completed.\n",
      "Epoch 76/100 completed.\n",
      "Epoch 77/100 completed.\n",
      "Epoch 78/100 completed.\n",
      "Epoch 79/100 completed.\n",
      "Epoch 80/100 completed.\n",
      "Epoch 81/100 completed.\n",
      "Epoch 82/100 completed.\n",
      "Epoch 83/100 completed.\n",
      "Epoch 84/100 completed.\n",
      "Epoch 85/100 completed.\n",
      "Epoch 86/100 completed.\n",
      "Epoch 87/100 completed.\n",
      "Epoch 88/100 completed.\n",
      "Epoch 89/100 completed.\n",
      "Epoch 90/100 completed.\n",
      "Epoch 91/100 completed.\n",
      "Epoch 92/100 completed.\n",
      "Epoch 93/100 completed.\n",
      "Epoch 94/100 completed.\n",
      "Epoch 95/100 completed.\n",
      "Epoch 96/100 completed.\n",
      "Epoch 97/100 completed.\n",
      "Epoch 98/100 completed.\n",
      "Epoch 99/100 completed.\n",
      "Epoch 100/100 completed.\n"
     ]
    }
   ],
   "source": [
    "# use cuda if available\n",
    "model = model.to(device)\n",
    "train(model, data_loader, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"eeg2vec/data/saved_models/eeg2vec_2_smaller_400windows.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EEG2Vec:\n\tMissing key(s) in state_dict: \"transformer_encoder.transformer_encoder.layers.3.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.3.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.3.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.3.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.3.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.3.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.3.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.3.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.3.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.3.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.3.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.3.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.4.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.4.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.4.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.4.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.4.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.4.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.4.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.4.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.4.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.4.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.4.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.4.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.5.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.5.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.5.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.5.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.5.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.5.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.5.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.5.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.5.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.5.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.5.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.5.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.6.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.6.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.6.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.6.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.6.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.6.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.6.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.6.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.6.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.6.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.6.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.6.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.7.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.7.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.7.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.7.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.7.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.7.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.7.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.7.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.7.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.7.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.7.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.7.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.8.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.8.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.8.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.8.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.8.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.8.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.8.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.8.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.8.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.8.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.8.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.8.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.9.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.9.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.9.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.9.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.9.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.9.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.9.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.9.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.9.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.9.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.9.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.9.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.10.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.10.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.10.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.10.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.10.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.10.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.10.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.10.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.10.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.10.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.10.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.10.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.11.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.11.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.11.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.11.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.11.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.11.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.11.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.11.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.11.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.11.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.11.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.11.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.12.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.12.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.12.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.12.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.12.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.12.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.12.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.12.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.12.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.12.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.12.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.12.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.13.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.13.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.13.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.13.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.13.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.13.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.13.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.13.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.13.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.13.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.13.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.13.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.14.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.14.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.14.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.14.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.14.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.14.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.14.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.14.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.14.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.14.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.14.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.14.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.15.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.15.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.15.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.15.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.15.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.15.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.15.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.15.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.15.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.15.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.15.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.15.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.16.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.16.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.16.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.16.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.16.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.16.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.16.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.16.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.16.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.16.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.16.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.16.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.17.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.17.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.17.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.17.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.17.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.17.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.17.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.17.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.17.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.17.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.17.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.17.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.18.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.18.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.18.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.18.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.18.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.18.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.18.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.18.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.18.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.18.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.18.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.18.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.19.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.19.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.19.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.19.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.19.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.19.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.19.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.19.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.19.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.19.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.19.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.19.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.20.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.20.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.20.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.20.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.20.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.20.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.20.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.20.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.20.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.20.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.20.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.20.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.21.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.21.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.21.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.21.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.21.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.21.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.21.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.21.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.21.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.21.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.21.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.21.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.22.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.22.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.22.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.22.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.22.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.22.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.22.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.22.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.22.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.22.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.22.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.22.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.23.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.23.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.23.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.23.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.23.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.23.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.23.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.23.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.23.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.23.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.23.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.23.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.24.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.24.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.24.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.24.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.24.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.24.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.24.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.24.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.24.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.24.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.24.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.24.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.25.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.25.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.25.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.25.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.25.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.25.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.25.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.25.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.25.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.25.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.25.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.25.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.26.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.26.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.26.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.26.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.26.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.26.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.26.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.26.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.26.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.26.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.26.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.26.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.27.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.27.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.27.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.27.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.27.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.27.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.27.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.27.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.27.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.27.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.27.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.27.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.28.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.28.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.28.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.28.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.28.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.28.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.28.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.28.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.28.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.28.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.28.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.28.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.29.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.29.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.29.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.29.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.29.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.29.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.29.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.29.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.29.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.29.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.29.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.29.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.30.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.30.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.30.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.30.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.30.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.30.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.30.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.30.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.30.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.30.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.30.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.30.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.31.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.31.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.31.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.31.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.31.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.31.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.31.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.31.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.31.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.31.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.31.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.31.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.32.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.32.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.32.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.32.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.32.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.32.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.32.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.32.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.32.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.32.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.32.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.32.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.33.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.33.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.33.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.33.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.33.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.33.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.33.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.33.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.33.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.33.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.33.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.33.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.34.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.34.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.34.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.34.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.34.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.34.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.34.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.34.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.34.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.34.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.34.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.34.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.35.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.35.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.35.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.35.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.35.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.35.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.35.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.35.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.35.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.35.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.35.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.35.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.36.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.36.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.36.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.36.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.36.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.36.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.36.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.36.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.36.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.36.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.36.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.36.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.37.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.37.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.37.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.37.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.37.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.37.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.37.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.37.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.37.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.37.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.37.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.37.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.38.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.38.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.38.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.38.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.38.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.38.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.38.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.38.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.38.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.38.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.38.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.38.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.39.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.39.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.39.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.39.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.39.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.39.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.39.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.39.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.39.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.39.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.39.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.39.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.40.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.40.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.40.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.40.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.40.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.40.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.40.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.40.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.40.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.40.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.40.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.40.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.41.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.41.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.41.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.41.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.41.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.41.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.41.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.41.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.41.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.41.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.41.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.41.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.42.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.42.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.42.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.42.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.42.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.42.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.42.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.42.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.42.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.42.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.42.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.42.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.43.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.43.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.43.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.43.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.43.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.43.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.43.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.43.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.43.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.43.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.43.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.43.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.44.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.44.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.44.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.44.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.44.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.44.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.44.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.44.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.44.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.44.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.44.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.44.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.45.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.45.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.45.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.45.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.45.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.45.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.45.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.45.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.45.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.45.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.45.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.45.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.46.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.46.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.46.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.46.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.46.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.46.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.46.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.46.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.46.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.46.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.46.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.46.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.47.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.47.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.47.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.47.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.47.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.47.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.47.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.47.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.47.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.47.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.47.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.47.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.48.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.48.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.48.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.48.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.48.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.48.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.48.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.48.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.48.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.48.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.48.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.48.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.49.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.49.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.49.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.49.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.49.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.49.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.49.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.49.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.49.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.49.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.49.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.49.norm2.bias\". \n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([48, 16]) from checkpoint, the shape in current model is torch.Size([192, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([16, 16]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2, 16]) from checkpoint, the shape in current model is torch.Size([2, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([16, 2]) from checkpoint, the shape in current model is torch.Size([64, 2]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([48, 16]) from checkpoint, the shape in current model is torch.Size([192, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([16, 16]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2, 16]) from checkpoint, the shape in current model is torch.Size([2, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([16, 2]) from checkpoint, the shape in current model is torch.Size([64, 2]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([48, 16]) from checkpoint, the shape in current model is torch.Size([192, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([16, 16]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([2, 16]) from checkpoint, the shape in current model is torch.Size([2, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([16, 2]) from checkpoint, the shape in current model is torch.Size([64, 2]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m EEG2Vec(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meeg2vec/data/saved_models/eeg2vec_2_smaller_400windows.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EEG2Vec:\n\tMissing key(s) in state_dict: \"transformer_encoder.transformer_encoder.layers.3.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.3.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.3.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.3.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.3.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.3.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.3.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.3.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.3.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.3.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.3.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.3.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.4.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.4.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.4.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.4.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.4.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.4.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.4.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.4.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.4.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.4.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.4.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.4.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.5.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.5.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.5.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.5.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.5.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.5.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.5.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.5.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.5.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.5.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.5.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.5.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.6.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.6.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.6.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.6.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.6.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.6.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.6.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.6.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.6.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.6.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.6.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.6.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.7.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.7.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.7.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.7.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.7.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.7.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.7.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.7.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.7.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.7.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.7.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.7.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.8.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.8.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.8.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.8.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.8.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.8.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.8.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.8.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.8.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.8.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.8.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.8.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.9.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.9.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.9.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.9.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.9.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.9.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.9.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.9.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.9.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.9.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.9.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.9.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.10.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.10.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.10.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.10.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.10.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.10.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.10.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.10.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.10.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.10.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.10.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.10.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.11.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.11.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.11.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.11.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.11.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.11.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.11.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.11.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.11.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.11.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.11.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.11.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.12.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.12.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.12.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.12.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.12.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.12.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.12.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.12.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.12.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.12.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.12.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.12.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.13.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.13.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.13.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.13.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.13.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.13.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.13.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.13.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.13.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.13.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.13.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.13.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.14.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.14.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.14.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.14.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.14.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.14.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.14.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.14.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.14.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.14.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.14.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.14.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.15.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.15.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.15.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.15.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.15.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.15.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.15.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.15.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.15.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.15.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.15.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.15.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.16.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.16.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.16.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.16.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.16.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.16.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.16.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.16.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.16.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.16.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.16.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.16.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.17.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.17.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.17.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.17.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.17.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.17.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.17.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.17.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.17.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.17.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.17.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.17.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.18.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.18.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.18.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.18.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.18.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.18.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.18.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.18.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.18.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.18.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.18.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.18.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.19.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.19.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.19.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.19.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.19.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.19.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.19.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.19.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.19.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.19.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.19.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.19.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.20.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.20.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.20.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.20.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.20.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.20.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.20.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.20.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.20.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.20.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.20.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.20.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.21.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.21.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.21.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.21.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.21.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.21.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.21.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.21.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.21.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.21.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.21.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.21.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.22.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.22.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.22.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.22.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.22.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.22.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.22.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.22.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.22.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.22.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.22.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.22.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.23.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.23.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.23.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.23.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.23.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.23.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.23.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.23.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.23.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.23.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.23.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.23.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.24.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.24.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.24.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.24.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.24.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.24.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.24.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.24.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.24.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.24.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.24.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.24.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.25.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.25.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.25.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.25.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.25.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.25.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.25.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.25.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.25.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.25.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.25.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.25.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.26.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.26.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.26.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.26.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.26.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.26.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.26.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.26.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.26.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.26.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.26.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.26.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.27.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.27.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.27.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.27.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.27.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.27.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.27.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.27.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.27.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.27.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.27.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.27.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.28.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.28.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.28.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.28.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.28.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.28.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.28.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.28.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.28.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.28.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.28.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.28.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.29.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.29.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.29.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.29.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.29.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.29.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.29.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.29.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.29.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.29.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.29.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.29.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.30.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.30.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.30.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.30.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.30.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.30.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.30.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.30.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.30.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.30.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.30.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.30.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.31.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.31.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.31.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.31.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.31.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.31.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.31.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.31.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.31.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.31.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.31.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.31.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.32.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.32.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.32.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.32.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.32.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.32.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.32.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.32.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.32.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.32.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.32.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.32.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.33.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.33.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.33.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.33.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.33.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.33.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.33.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.33.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.33.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.33.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.33.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.33.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.34.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.34.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.34.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.34.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.34.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.34.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.34.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.34.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.34.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.34.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.34.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.34.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.35.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.35.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.35.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.35.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.35.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.35.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.35.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.35.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.35.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.35.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.35.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.35.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.36.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.36.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.36.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.36.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.36.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.36.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.36.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.36.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.36.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.36.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.36.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.36.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.37.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.37.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.37.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.37.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.37.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.37.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.37.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.37.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.37.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.37.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.37.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.37.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.38.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.38.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.38.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.38.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.38.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.38.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.38.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.38.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.38.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.38.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.38.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.38.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.39.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.39.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.39.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.39.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.39.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.39.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.39.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.39.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.39.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.39.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.39.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.39.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.40.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.40.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.40.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.40.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.40.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.40.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.40.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.40.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.40.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.40.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.40.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.40.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.41.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.41.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.41.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.41.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.41.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.41.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.41.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.41.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.41.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.41.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.41.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.41.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.42.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.42.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.42.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.42.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.42.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.42.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.42.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.42.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.42.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.42.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.42.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.42.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.43.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.43.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.43.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.43.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.43.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.43.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.43.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.43.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.43.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.43.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.43.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.43.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.44.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.44.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.44.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.44.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.44.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.44.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.44.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.44.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.44.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.44.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.44.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.44.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.45.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.45.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.45.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.45.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.45.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.45.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.45.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.45.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.45.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.45.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.45.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.45.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.46.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.46.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.46.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.46.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.46.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.46.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.46.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.46.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.46.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.46.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.46.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.46.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.47.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.47.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.47.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.47.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.47.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.47.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.47.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.47.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.47.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.47.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.47.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.47.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.48.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.48.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.48.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.48.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.48.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.48.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.48.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.48.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.48.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.48.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.48.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.48.norm2.bias\", \"transformer_encoder.transformer_encoder.layers.49.self_attn.in_proj_weight\", \"transformer_encoder.transformer_encoder.layers.49.self_attn.in_proj_bias\", \"transformer_encoder.transformer_encoder.layers.49.self_attn.out_proj.weight\", \"transformer_encoder.transformer_encoder.layers.49.self_attn.out_proj.bias\", \"transformer_encoder.transformer_encoder.layers.49.linear1.weight\", \"transformer_encoder.transformer_encoder.layers.49.linear1.bias\", \"transformer_encoder.transformer_encoder.layers.49.linear2.weight\", \"transformer_encoder.transformer_encoder.layers.49.linear2.bias\", \"transformer_encoder.transformer_encoder.layers.49.norm1.weight\", \"transformer_encoder.transformer_encoder.layers.49.norm1.bias\", \"transformer_encoder.transformer_encoder.layers.49.norm2.weight\", \"transformer_encoder.transformer_encoder.layers.49.norm2.bias\". \n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([48, 16]) from checkpoint, the shape in current model is torch.Size([192, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([16, 16]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2, 16]) from checkpoint, the shape in current model is torch.Size([2, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.linear2.weight: copying a param with shape torch.Size([16, 2]) from checkpoint, the shape in current model is torch.Size([64, 2]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.linear2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.0.norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([48, 16]) from checkpoint, the shape in current model is torch.Size([192, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([16, 16]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2, 16]) from checkpoint, the shape in current model is torch.Size([2, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.linear2.weight: copying a param with shape torch.Size([16, 2]) from checkpoint, the shape in current model is torch.Size([64, 2]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.linear2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.1.norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([48, 16]) from checkpoint, the shape in current model is torch.Size([192, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([16, 16]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.linear1.weight: copying a param with shape torch.Size([2, 16]) from checkpoint, the shape in current model is torch.Size([2, 64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.linear2.weight: copying a param with shape torch.Size([16, 2]) from checkpoint, the shape in current model is torch.Size([64, 2]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.linear2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.norm1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.norm1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.norm2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for transformer_encoder.transformer_encoder.layers.2.norm2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"eeg2vec/data/saved_models/eeg2vec_2_smaller_400windows.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for xgboost training data\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    training_embeddings = model(torch.tensor(X_train_xgboost, dtype=torch.float32).to(device))\n",
    "    test_embeddings = model(torch.tensor(X_test, dtype=torch.float32).to(device))\n",
    "    test_embeddings = test_embeddings.cpu().numpy()\n",
    "    training_embeddings = training_embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 248, 16)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # For binary classification; use 'multi:softmax' for multi-class\n",
    "    'eval_metric': 'logloss',        # Evaluation metric (logarithmic loss)\n",
    "    'learning_rate': 0.1,            # Step size shrinkage\n",
    "    'max_depth': 6,                  # Maximum tree depth\n",
    "    'subsample': 0.8,                # Percentage of samples to use per tree\n",
    "    'colsample_bytree': 0.8,         # Percentage of features to use per tree\n",
    "    'lambda': 1,                     # L2 regularization term\n",
    "    'alpha': 0                       # L1 regularization term\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model_xgb = MultiOutputClassifier(xgb.XGBClassifier(**params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 5)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_xgboost.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1])]\n"
     ]
    }
   ],
   "source": [
    "print(model_xgb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_embeddings = training_embeddings.reshape(training_embeddings.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=XGBClassifier(alpha=0, base_score=None,\n",
       "                                              booster=None, callbacks=None,\n",
       "                                              colsample_bylevel=None,\n",
       "                                              colsample_bynode=None,\n",
       "                                              colsample_bytree=0.8, device=None,\n",
       "                                              early_stopping_rounds=None,\n",
       "                                              enable_categorical=False,\n",
       "                                              eval_metric=&#x27;logloss&#x27;,\n",
       "                                              feature_types=None, gamma=None,\n",
       "                                              grow_policy=None,\n",
       "                                              importance_type=None,\n",
       "                                              interaction_constraints=None,\n",
       "                                              lambda=1, learning_rate=0.1,\n",
       "                                              max_bin=None,\n",
       "                                              max_cat_threshold=None,\n",
       "                                              max_cat_to_onehot=None,\n",
       "                                              max_delta_step=None, max_depth=6,\n",
       "                                              max_leaves=None,\n",
       "                                              min_child_weight=None,\n",
       "                                              missing=nan,\n",
       "                                              monotone_constraints=None,\n",
       "                                              multi_strategy=None,\n",
       "                                              n_estimators=None, n_jobs=None, ...))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultiOutputClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\">?<span>Documentation for MultiOutputClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputClassifier(estimator=XGBClassifier(alpha=0, base_score=None,\n",
       "                                              booster=None, callbacks=None,\n",
       "                                              colsample_bylevel=None,\n",
       "                                              colsample_bynode=None,\n",
       "                                              colsample_bytree=0.8, device=None,\n",
       "                                              early_stopping_rounds=None,\n",
       "                                              enable_categorical=False,\n",
       "                                              eval_metric=&#x27;logloss&#x27;,\n",
       "                                              feature_types=None, gamma=None,\n",
       "                                              grow_policy=None,\n",
       "                                              importance_type=None,\n",
       "                                              interaction_constraints=None,\n",
       "                                              lambda=1, learning_rate=0.1,\n",
       "                                              max_bin=None,\n",
       "                                              max_cat_threshold=None,\n",
       "                                              max_cat_to_onehot=None,\n",
       "                                              max_delta_step=None, max_depth=6,\n",
       "                                              max_leaves=None,\n",
       "                                              min_child_weight=None,\n",
       "                                              missing=nan,\n",
       "                                              monotone_constraints=None,\n",
       "                                              multi_strategy=None,\n",
       "                                              n_estimators=None, n_jobs=None, ...))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(alpha=0, base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None, lambda=1,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, ...)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">XGBClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(alpha=0, base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None, lambda=1,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, ...)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputClassifier(estimator=XGBClassifier(alpha=0, base_score=None,\n",
       "                                              booster=None, callbacks=None,\n",
       "                                              colsample_bylevel=None,\n",
       "                                              colsample_bynode=None,\n",
       "                                              colsample_bytree=0.8, device=None,\n",
       "                                              early_stopping_rounds=None,\n",
       "                                              enable_categorical=False,\n",
       "                                              eval_metric='logloss',\n",
       "                                              feature_types=None, gamma=None,\n",
       "                                              grow_policy=None,\n",
       "                                              importance_type=None,\n",
       "                                              interaction_constraints=None,\n",
       "                                              lambda=1, learning_rate=0.1,\n",
       "                                              max_bin=None,\n",
       "                                              max_cat_threshold=None,\n",
       "                                              max_cat_to_onehot=None,\n",
       "                                              max_delta_step=None, max_depth=6,\n",
       "                                              max_leaves=None,\n",
       "                                              min_child_weight=None,\n",
       "                                              missing=nan,\n",
       "                                              monotone_constraints=None,\n",
       "                                              multi_strategy=None,\n",
       "                                              n_estimators=None, n_jobs=None, ...))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb.fit(training_embeddings, y_train_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "pickle.dump(model_xgb, open(\"eeg2vec/data/saved_models/xgboost_2_smaller_400windows.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_xgb = pickle.load(open(\"eeg2vec/data/saved_models/xgboost_1_400windows.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "F1 Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_embeddings = test_embeddings.reshape(test_embeddings.shape[0], -1)\n",
    "predictions = model_xgb.predict(test_embeddings)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "# F1 score\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = all_data[7000:50000]\n",
    "test_targets = all_targets[7000:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n",
      "F1 Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "model =  model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i in range(0, len(test_data), 1000):\n",
    "        embeddings = model(torch.tensor(test_data[i:i+1000], dtype=torch.float32).to(device))\n",
    "        if i == 0:\n",
    "            all_embeddings = embeddings\n",
    "        else:\n",
    "            all_embeddings = torch.cat((all_embeddings, embeddings), dim=0)\n",
    "    embeddings = all_embeddings.reshape(all_embeddings.shape[0], -1).cpu().numpy()\n",
    "predictions = model_xgb.predict(embeddings)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(test_targets, predictions)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "# F1 score\n",
    "f1 = f1_score(test_targets, predictions, average='weighted')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to get the point that maps to a label\n",
    "\n",
    "def reshape_array_into_windows(x, sample_rate, window_duration_in_seconds):\n",
    "    \"\"\"\n",
    "    Reshape the data into an array of shape (C, T, window) where 'window' contains\n",
    "    the points corresponding to 'window_duration' seconds of data.\n",
    "\n",
    "    Parameters:\n",
    "    x (numpy array): The input data array.\n",
    "    sample_rate (int): The number of samples per second.\n",
    "    window_duration_in_seconds (float): The duration of each window in seconds.\n",
    "\n",
    "    Returns:\n",
    "    reshaped_x (numpy array): The reshaped array with shape (C, T, window).\n",
    "    \"\"\"\n",
    "    # Calculate the number of samples in one window\n",
    "    window_size = int(window_duration_in_seconds * sample_rate)\n",
    "    \n",
    "    # Ensure the total length of x is a multiple of window_size\n",
    "    total_samples = x.shape[-1]\n",
    "    if total_samples % window_size != 0:\n",
    "        # Truncate or pad x to make it divisible by window_size\n",
    "        x = x[..., :total_samples - (total_samples % window_size)]\n",
    "    # Reshape x into (C, T, window)\n",
    "    reshaped_x = x.reshape(x.shape[0], -1, window_size)\n",
    "\n",
    "    return reshaped_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first load and reshape all the data\n",
    "all_data = []\n",
    "all_targets = []\n",
    "# We need to have\n",
    "# data of Shape: [num_samples, num_channels (5), sequence_length]\n",
    "# labels of Shape: [num_samples, 5]\n",
    "\n",
    "for data, target in training_data:\n",
    "    reshaped_data = reshape_array_into_windows(data, 250, 2)\n",
    "    reshaped_data = reshaped_data.transpose(1, 0, 2)\n",
    "    target = target.reshape(-1, 5)\n",
    "    all_data.append(reshaped_data)\n",
    "    all_targets.append(target)\n",
    "\n",
    "all_data = np.concatenate(all_data, axis=0)\n",
    "all_targets = np.concatenate(all_targets, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52351, 5, 500)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(all_data, all_targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split training data for embeddings and XGBoost\n",
    "X_train_embeddings, X_train_xgboost, y_train_embeddings, y_train_xgboost = train_test_split(X_train_full, y_train_full, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEG2VEC training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..... Session 0 .....\n",
      "Epoch 1/20 completed.\n",
      "Epoch 2/20 completed.\n",
      "Epoch 3/20 completed.\n",
      "Epoch 4/20 completed.\n",
      "Epoch 5/20 completed.\n",
      "Epoch 6/20 completed.\n",
      "Epoch 7/20 completed.\n",
      "Epoch 8/20 completed.\n",
      "Epoch 9/20 completed.\n",
      "Epoch 10/20 completed.\n",
      "Epoch 11/20 completed.\n",
      "Epoch 12/20 completed.\n",
      "Epoch 13/20 completed.\n",
      "Epoch 14/20 completed.\n",
      "Epoch 15/20 completed.\n",
      "Epoch 16/20 completed.\n",
      "Epoch 17/20 completed.\n",
      "Epoch 18/20 completed.\n",
      "Epoch 19/20 completed.\n",
      "Epoch 20/20 completed.\n",
      "..... Session 1 .....\n",
      "Epoch 1/20 completed.\n",
      "Epoch 2/20 completed.\n",
      "Epoch 3/20 completed.\n",
      "Epoch 4/20 completed.\n",
      "Epoch 5/20 completed.\n",
      "Epoch 6/20 completed.\n",
      "Epoch 7/20 completed.\n",
      "Epoch 8/20 completed.\n",
      "Epoch 9/20 completed.\n",
      "Epoch 10/20 completed.\n",
      "Epoch 11/20 completed.\n",
      "Epoch 12/20 completed.\n",
      "Epoch 13/20 completed.\n",
      "Epoch 14/20 completed.\n",
      "Epoch 15/20 completed.\n",
      "Epoch 16/20 completed.\n",
      "Epoch 17/20 completed.\n",
      "Epoch 18/20 completed.\n",
      "Epoch 19/20 completed.\n",
      "Epoch 20/20 completed.\n",
      "..... Session 2 .....\n",
      "Epoch 1/20 completed.\n",
      "Epoch 2/20 completed.\n",
      "Epoch 3/20 completed.\n",
      "Epoch 4/20 completed.\n",
      "Epoch 5/20 completed.\n",
      "Epoch 6/20 completed.\n",
      "Epoch 7/20 completed.\n",
      "Epoch 8/20 completed.\n",
      "Epoch 9/20 completed.\n",
      "Epoch 10/20 completed.\n",
      "Epoch 11/20 completed.\n",
      "Epoch 12/20 completed.\n",
      "Epoch 13/20 completed.\n",
      "Epoch 14/20 completed.\n",
      "Epoch 15/20 completed.\n",
      "Epoch 16/20 completed.\n",
      "Epoch 17/20 completed.\n",
      "Epoch 18/20 completed.\n",
      "Epoch 19/20 completed.\n",
      "Epoch 20/20 completed.\n",
      "..... Session 3 .....\n",
      "Epoch 1/20 completed.\n",
      "Epoch 2/20 completed.\n",
      "Epoch 3/20 completed.\n",
      "Epoch 4/20 completed.\n",
      "Epoch 5/20 completed.\n",
      "Epoch 6/20 completed.\n",
      "Epoch 7/20 completed.\n",
      "Epoch 8/20 completed.\n",
      "Epoch 9/20 completed.\n",
      "Epoch 10/20 completed.\n",
      "Epoch 11/20 completed.\n",
      "Epoch 12/20 completed.\n",
      "Epoch 13/20 completed.\n",
      "Epoch 14/20 completed.\n",
      "Epoch 15/20 completed.\n",
      "Epoch 16/20 completed.\n",
      "Epoch 17/20 completed.\n",
      "Epoch 18/20 completed.\n",
      "Epoch 19/20 completed.\n",
      "Epoch 20/20 completed.\n",
      "Epoch 1/20 completed.\n",
      "Epoch 2/20 completed.\n",
      "Epoch 3/20 completed.\n",
      "Epoch 4/20 completed.\n",
      "Epoch 5/20 completed.\n",
      "Epoch 6/20 completed.\n",
      "Epoch 7/20 completed.\n",
      "Epoch 8/20 completed.\n",
      "Epoch 9/20 completed.\n",
      "Epoch 10/20 completed.\n",
      "Epoch 11/20 completed.\n",
      "Epoch 12/20 completed.\n",
      "Epoch 13/20 completed.\n",
      "Epoch 14/20 completed.\n",
      "Epoch 15/20 completed.\n",
      "Epoch 16/20 completed.\n",
      "Epoch 17/20 completed.\n",
      "Epoch 18/20 completed.\n",
      "Epoch 19/20 completed.\n",
      "Epoch 20/20 completed.\n"
     ]
    }
   ],
   "source": [
    "model_eeg2vec = EEG2Vec(16, 2, 3, 2)\n",
    "model_eeg2vec = model_eeg2vec.to(device)\n",
    "for session in range(4):\n",
    "    print(\"..... Session \" + str(session) + \" .....\")\n",
    "    session_length = int(len(X_train_embeddings)/5)\n",
    "    data_loader = get_dataloader(X_train_embeddings[session*session_length:(session+1)*session_length], y_train_embeddings[session*session_length:(session+1)*session_length], batch_size=100, shuffle=True)\n",
    "    train(model_eeg2vec, data_loader, 20, device)\n",
    "    # Save the model\n",
    "    torch.save(model_eeg2vec.state_dict(), \"eeg2vec/data/saved_models/eeg2vec_3_final.pth\")\n",
    "data_loader =get_dataloader(X_train_embeddings[(session+1)*session_length:], y_train_embeddings[(session+1)*session_length:], batch_size=100, shuffle=True)\n",
    "train(model_eeg2vec, data_loader, 20, device)\n",
    "torch.save(model_eeg2vec.state_dict(), \"eeg2vec/data/saved_models/eeg2vec_3_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 810.21 GiB (GPU 0; 11.73 GiB total capacity; 5.87 GiB already allocated; 3.68 GiB free; 6.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 6\u001b[0m training_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_xgboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m test_embeddings \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mtensor(X_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      8\u001b[0m test_embeddings \u001b[38;5;241m=\u001b[39m test_embeddings\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/eeg2vec/models/eeg2vec.py:25\u001b[0m, in \u001b[0;36mEEG2Vec.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_encoder(x)\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/eeg2vec/models/transformer_encoder.py:11\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/modules/transformer.py:238\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    236\u001b[0m         output \u001b[38;5;241m=\u001b[39m mod(output, src_mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    241\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m)\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/modules/transformer.py:463\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    464\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/modules/transformer.py:471\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    470\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 471\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/modules/activation.py:1153\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1143\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1151\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight, average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1153\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/functional.py:5179\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5174\u001b[0m     dropout_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m   5176\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   5177\u001b[0m \u001b[38;5;66;03m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[1;32m   5178\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m-> 5179\u001b[0m attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_scaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5180\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(tgt_len \u001b[38;5;241m*\u001b[39m bsz, embed_dim)\n\u001b[1;32m   5181\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "File \u001b[0;32m~/SleepEEGKaggle/Sleep_EEG_Kaggle/env/lib64/python3.9/site-packages/torch/nn/functional.py:4854\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   4852\u001b[0m     attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbaddbmm(attn_mask, q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   4853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4854\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4856\u001b[0m attn \u001b[38;5;241m=\u001b[39m softmax(attn, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 810.21 GiB (GPU 0; 11.73 GiB total capacity; 5.87 GiB already allocated; 3.68 GiB free; 6.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Compute embeddings for xgboost training data\n",
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    training_embeddings = model(torch.tensor(X_train_xgboost, dtype=torch.float32).to(device))\n",
    "    test_embeddings = model(torch.tensor(X_test, dtype=torch.float32).to(device))\n",
    "    test_embeddings = test_embeddings.cpu().numpy()\n",
    "    training_embeddings = training_embeddings.cpu().numpy()\n",
    "training_embeddings = training_embeddings.reshape(training_embeddings.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',  # For binary classification; use 'multi:softmax' for multi-class\n",
    "    'eval_metric': 'logloss',        # Evaluation metric (logarithmic loss)\n",
    "    'learning_rate': 0.1,            # Step size shrinkage\n",
    "    'max_depth': 6,                  # Maximum tree depth\n",
    "    'subsample': 0.8,                # Percentage of samples to use per tree\n",
    "    'colsample_bytree': 0.8,         # Percentage of features to use per tree\n",
    "    'lambda': 1,                     # L2 regularization term\n",
    "    'alpha': 0                       # L1 regularization term\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model_xgb_eeg2vec = MultiOutputClassifier(xgb.XGBClassifier(**params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=XGBClassifier(alpha=0, base_score=None,\n",
       "                                              booster=None, callbacks=None,\n",
       "                                              colsample_bylevel=None,\n",
       "                                              colsample_bynode=None,\n",
       "                                              colsample_bytree=0.8, device=None,\n",
       "                                              early_stopping_rounds=None,\n",
       "                                              enable_categorical=False,\n",
       "                                              eval_metric=&#x27;logloss&#x27;,\n",
       "                                              feature_types=None, gamma=None,\n",
       "                                              grow_policy=None,\n",
       "                                              importance_type=None,\n",
       "                                              interaction_constraints=None,\n",
       "                                              lambda=1, learning_rate=0.1,\n",
       "                                              max_bin=None,\n",
       "                                              max_cat_threshold=None,\n",
       "                                              max_cat_to_onehot=None,\n",
       "                                              max_delta_step=None, max_depth=6,\n",
       "                                              max_leaves=None,\n",
       "                                              min_child_weight=None,\n",
       "                                              missing=nan,\n",
       "                                              monotone_constraints=None,\n",
       "                                              multi_strategy=None,\n",
       "                                              n_estimators=None, n_jobs=None, ...))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultiOutputClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\">?<span>Documentation for MultiOutputClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputClassifier(estimator=XGBClassifier(alpha=0, base_score=None,\n",
       "                                              booster=None, callbacks=None,\n",
       "                                              colsample_bylevel=None,\n",
       "                                              colsample_bynode=None,\n",
       "                                              colsample_bytree=0.8, device=None,\n",
       "                                              early_stopping_rounds=None,\n",
       "                                              enable_categorical=False,\n",
       "                                              eval_metric=&#x27;logloss&#x27;,\n",
       "                                              feature_types=None, gamma=None,\n",
       "                                              grow_policy=None,\n",
       "                                              importance_type=None,\n",
       "                                              interaction_constraints=None,\n",
       "                                              lambda=1, learning_rate=0.1,\n",
       "                                              max_bin=None,\n",
       "                                              max_cat_threshold=None,\n",
       "                                              max_cat_to_onehot=None,\n",
       "                                              max_delta_step=None, max_depth=6,\n",
       "                                              max_leaves=None,\n",
       "                                              min_child_weight=None,\n",
       "                                              missing=nan,\n",
       "                                              monotone_constraints=None,\n",
       "                                              multi_strategy=None,\n",
       "                                              n_estimators=None, n_jobs=None, ...))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(alpha=0, base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None, lambda=1,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, ...)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">XGBClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(alpha=0, base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None, lambda=1,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, ...)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputClassifier(estimator=XGBClassifier(alpha=0, base_score=None,\n",
       "                                              booster=None, callbacks=None,\n",
       "                                              colsample_bylevel=None,\n",
       "                                              colsample_bynode=None,\n",
       "                                              colsample_bytree=0.8, device=None,\n",
       "                                              early_stopping_rounds=None,\n",
       "                                              enable_categorical=False,\n",
       "                                              eval_metric='logloss',\n",
       "                                              feature_types=None, gamma=None,\n",
       "                                              grow_policy=None,\n",
       "                                              importance_type=None,\n",
       "                                              interaction_constraints=None,\n",
       "                                              lambda=1, learning_rate=0.1,\n",
       "                                              max_bin=None,\n",
       "                                              max_cat_threshold=None,\n",
       "                                              max_cat_to_onehot=None,\n",
       "                                              max_delta_step=None, max_depth=6,\n",
       "                                              max_leaves=None,\n",
       "                                              min_child_weight=None,\n",
       "                                              missing=nan,\n",
       "                                              monotone_constraints=None,\n",
       "                                              multi_strategy=None,\n",
       "                                              n_estimators=None, n_jobs=None, ...))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_xgb_eeg2vec.fit(training_embeddings, y_train_xgboost)\n",
    "pickle.dump(model_xgb, open(\"eeg2vec/data/saved_models/xgboost_3_smaller_final.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_embeddings = test_embeddings.reshape(test_embeddings.shape[0], -1)\n",
    "predictions = model_xgb.predict(test_embeddings)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "# F1 score\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_TEST_PATH = Path(\"test/\")\n",
    "test_data = {i:np.load(ROOT_TEST_PATH / f\"data_{i}.npy\") for i in [4,5]}\n",
    "# We process each record independantly\n",
    "\n",
    "def compute_predictions_on_record(data,model,model_xgb):\n",
    "    filtered_data =  butter_bandpass_filter(data,0.1,18,250,4)\n",
    "    reshaped_data = reshape_array_into_windows(filtered_data,250,2)\n",
    "\n",
    "    reshaped_data = reshaped_data.transpose(1, 0, 2)\n",
    "    model =  model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = model(torch.tensor(reshaped_data, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "    embeddings = embeddings.reshape(embeddings.shape[0], -1)\n",
    "    predictions = model_xgb.predict(embeddings)\n",
    "    return predictions\n",
    "\n",
    "def format_array_to_target_format(array, record_number):\n",
    "    assert isinstance(record_number, int)\n",
    "    assert isinstance(array, np.ndarray)\n",
    "    assert len(array.shape) == 2\n",
    "    assert array.shape[0] == 5\n",
    "    assert set(np.unique(array)) == {0, 1}\n",
    "    formatted_target = []\n",
    "    for i in range(array.shape[0]):\n",
    "        channel_encoding = (i + 1) * 100000\n",
    "        record_number_encoding = record_number * 1000000\n",
    "        for j in range(array.shape[1]):\n",
    "            formatted_target.append(\n",
    "                {\n",
    "                    \"identifier\": record_number_encoding + channel_encoding + j,\n",
    "                    \"target\": array[i, j],\n",
    "                }\n",
    "            )\n",
    "    return formatted_target\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6602015)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for record_number, data in test_data.items():\n",
    "    with torch.no_grad():\n",
    "        preds = compute_predictions_on_record(data,model,model_xgb)\n",
    "    formatted_preds = format_array_to_target_format(preds,record_number)\n",
    "    results.extend(formatted_preds)\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"submission.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
